{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "parallel-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.delegate import ASGLoadDelegate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "isolated-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers import GlobalParams\n",
    "from data.collate_fns import convert_batch_sparse_matrix_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "searching-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thexp import globs \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comic-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = GlobalParams()\n",
    "# params.dim_fts = [2048,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becoming-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataxy import mscoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vulnerable-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas =mscoco('test',set_root='asg2cap/ControllableImageCaption/MSCOCO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accurate-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_ids, json_fs, hdf5_fs, mp_ids, mp_fts = datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fantastic-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoimg_root = '/home/yanghaozhe/datasets/COCO/2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aquatic-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_fn = json_fs[2].strip('.json')\n",
    "\n",
    "img_path = os.path.join(cocoimg_root,'{}.jpg'.format('/'.join(json_fn.split('/')[-2:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "important-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = dict(next(iter(loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "informational-alias",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   3,  36,  79,   4,  29,  64,   9,   3, 105,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(kk['caption_ids']).astype(np.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "seasonal-calvin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2048])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk['mp_fts'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "diverse-collection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 6, 10, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk['rel_edges'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unable-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.mixin.models import AsgModelMixin\n",
    "from trainers.mixin.datasets import BaseSupDatasetMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "antique-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.asg2cap import BaseTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "authorized-extension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add output channel on /home/yanghaozhe/.thexp/experiments/asg2cap.e0/asg2cap.trainers/0001.a1cefe81/l.210205164028.0.log\n",
      "/opt/conda/lib/python3.7/site-packages/xpython_launcher.py -f /root/.local/share/jupyter/runtime/kernel-acbcfbcb-0ee2-4a71-ac64-a4d4d35cd6d4.json | \n",
      "21-02-05 16:40:28 | Exp BaseDir | /home/yanghaozhe/.thexp/experiments/asg2cap.e0/asg2cap.trainers\n",
      "21-02-05 16:40:28 | Exp Trainer | BaseTrainer\n",
      "21-02-05 16:40:28 | Exp Params\n",
      "GlobalParams[('epoch', 100),\n",
      " ('eidx', 1),\n",
      " ('idx', 0),\n",
      " ('global_step', 0),\n",
      " ('device', 'cuda:0'),\n",
      " ('device_ids', []),\n",
      " ('dataset', 'mscoco'),\n",
      " ('architecture', 'rgcn.flow.memory'),\n",
      " ('optim',\n",
      "  OptimParams([('args', attr([('lr', 0.0002), ('weight_decay', 0)])),\n",
      "               ('name', 'Adam')])),\n",
      " ('git_commit', True),\n",
      " ('tmp_dir', None),\n",
      " ('distributed', False),\n",
      " ('world_size', -1),\n",
      " ('local_rank', -1),\n",
      " ('init_method', 'env://'),\n",
      " ('attn_encoder', 'role_rgcn'),\n",
      " ('decoder', 'memory_flow'),\n",
      " ('n_classes', 10),\n",
      " ('topk', (1, 2, 3, 4)),\n",
      " ('batch_size', 128),\n",
      " ('num_workers', 8),\n",
      " ('ema', False),\n",
      " ('ema_alpha', 0.999),\n",
      " ('val_size', 10000),\n",
      " ('max_attn_len', 10),\n",
      " ('pixel_reduce', 1),\n",
      " ('dim_hidden', 512),\n",
      " ('dim_input', 2048),\n",
      " ('dropout', 0),\n",
      " ('embed_first', True),\n",
      " ('freeze', False),\n",
      " ('num_hidden_layers', 2),\n",
      " ('num_node_types', 3),\n",
      " ('num_rels', 6),\n",
      " ('self_loop', True),\n",
      " ('weight_decay', 0),\n",
      " ('attn_input_size', 512),\n",
      " ('attn_size', 512),\n",
      " ('attn_type', 'mlp'),\n",
      " ('beam_width', 1),\n",
      " ('dim_word', 512),\n",
      " ('fix_word_embed', False),\n",
      " ('greedy_or_beam', True),\n",
      " ('hidden2word', False),\n",
      " ('hidden_size', 512),\n",
      " ('max_words_in_sent', 25),\n",
      " ('memory_same_key_value', True),\n",
      " ('num_layers', 1),\n",
      " ('num_words', 10942),\n",
      " ('schedule_sampling', False),\n",
      " ('sent_pool_size', 1),\n",
      " ('ss_increase_epoch', 5),\n",
      " ('ss_increase_rate', 0.05),\n",
      " ('ss_max_rate', 0.25),\n",
      " ('ss_rate', 0.0),\n",
      " ('tie_embed', True),\n",
      " ('dim_embed', 512),\n",
      " ('dim_fts', [2048, 512]),\n",
      " ('is_embed', True),\n",
      " ('lr_mult', 1.0),\n",
      " ('nonlinear', False),\n",
      " ('norm', False),\n",
      " ('state_dict', attr())] | \n",
      "21-02-05 16:40:28 | <LoggerCallback([priority=100]) at 0x00007F87F25DC990> hooked on <trainers.asg2cap.BaseTrainer object at 0x7f87f2624f50>.\n",
      "21-02-05 16:40:28 | <EvalCallback([eval_in_per_epoch=5; test_per_epoch=None]) at 0x00007F87F1EAB3D0> hooked on <trainers.asg2cap.BaseTrainer object at 0x7f87f2624f50>.\n",
      "21-02-05 16:40:28 | <AutoRecord([priority=100]) at 0x00007F87F1C70DD0> hooked on <trainers.asg2cap.BaseTrainer object at 0x7f87f2624f50>.\n",
      "21-02-05 16:40:28 | <TimingCheckpoint([per_epoch=10]) at 0x00007F87F25B7AD0> hooked on <trainers.asg2cap.BaseTrainer object at 0x7f87f2624f50>.\n",
      "21-02-05 16:40:28 | WARN | lr_sche not exists in params and be assigned, {} will be unhooked after.\n",
      "21-02-05 16:40:28 | <LRSchedule([priority=0]) at 0x00007F87F1C70E10> unhooked from <trainers.asg2cap.BaseTrainer object at 0x7f87f2624f50>.\n",
      "21-02-05 16:40:28 | <LRSchedule([priority=0]) at 0x00007F87F1C70E10> hooked on <trainers.asg2cap.BaseTrainer object at 0x7f87f2624f50>.\n",
      "21-02-05 16:40:52 | {'train': DataBundler([('unnamed_0', 3711)]), 'eval': DataBundler([('unnamed_0', 162)]), 'tests': DataBundler([('unnamed_0', 162)])}\n"
     ]
    }
   ],
   "source": [
    "trainer = BaseTrainer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "classical-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = trainer.test_dataloader.choice_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "theoretical-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thexp import Meter\n",
    "import torch\n",
    "# from ..asg2cap.data.transforms import Int2Sent\n",
    "\n",
    "params.greedy_or_beam = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "virtual-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_root = '/root/.thexp/experiments/asg2cap.e0/asg2cap.trainers/0011.b632f246/modules'\n",
    "\n",
    "model_pt = os.path.join(model_root,'keep.0000030.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "australian-reporter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint | \n",
      "21-02-05 16:44:43 | loading model: model | \n",
      "21-02-05 16:44:43 | loading optimizers: optim | \n",
      "21-02-05 16:44:43 | loading other: 21-02-05 16:44:43 | loading vectors: {'Lall': '0.7316(0.6821)',\n",
      " 'Lcap': '0.7316(0.6821)',\n",
      " 'fn': '/root/.thexp/experiments/asg2cap.e0/asg2cap.trainers/0011.b632f246/modules/keep.0000030.ckpt'} | \n"
     ]
    }
   ],
   "source": [
    "trainer.load_checkpoint(model_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "broken-greene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7238018162070041, array([6.83237863e-01, 3.18185131e+00, 5.17484718e-02, 7.13428363e+00,\n",
       "       8.33883776e-01, 3.05561714e-01, 4.62660104e-01, 7.86409468e-02,\n",
       "       2.80662551e+00, 9.43192422e-01, 1.15172792e+00, 8.87019579e-01,\n",
       "       0.00000000e+00, 1.65409347e-01, 4.75902650e-01, 9.19417308e-01,\n",
       "       7.47406924e-01, 3.82002369e-01, 2.71265771e-01, 1.40136456e-01,\n",
       "       2.40561105e-01, 1.31271006e-01, 3.03626584e-02, 3.21047357e+00,\n",
       "       5.20694049e-01, 1.68328223e+00, 2.33831933e+00, 6.38795028e-04,\n",
       "       1.19781988e-03, 7.23874857e-01, 4.48683638e-04, 1.64965453e-03,\n",
       "       1.25503996e+00, 8.20668839e-01, 1.57341928e+00, 3.39825254e+00,\n",
       "       6.10603517e-01, 2.25603177e+00, 4.41351208e+00, 6.15252939e+00,\n",
       "       5.55018932e+00, 3.39923842e-01, 1.68002735e+00, 9.54076207e+00,\n",
       "       4.66242914e+00, 1.78375771e+00, 3.59560486e-01, 4.01594241e-02,\n",
       "       2.49685251e+00, 8.84389414e-01, 1.87855989e+00, 1.18551076e+00,\n",
       "       1.95290753e-01, 2.11314051e+00, 1.78018592e-01, 2.09933775e+00,\n",
       "       4.03989425e-01, 1.63283968e-01, 3.28838417e-01, 5.46286765e-01,\n",
       "       5.86779195e-01, 2.29073282e+00, 1.28582180e-01, 4.99392519e-01,\n",
       "       2.76890902e+00, 1.11591168e+00, 6.22967300e-01, 8.52503662e-01,\n",
       "       2.41225846e+00, 5.28317712e+00, 3.09703787e-01, 6.92898913e+00,\n",
       "       2.51321737e+00, 4.69048042e+00, 1.73574873e+00, 1.02703870e+00,\n",
       "       9.52386774e-01, 8.36045826e+00, 2.17033499e+00, 4.65989361e+00,\n",
       "       4.29085356e-01, 8.25033917e-01, 1.46012766e+00, 3.24928693e+00,\n",
       "       2.88745182e+00, 9.93441566e-01, 2.92004640e-01, 5.38662890e-01,\n",
       "       1.71913507e-01, 2.37463640e+00, 4.47290516e+00, 2.67222836e+00,\n",
       "       5.02074481e-01, 3.31278330e+00, 4.40476627e-04, 2.14517521e+00,\n",
       "       3.39160895e-01, 9.84825888e-01, 7.47211204e-01, 9.10704835e-01,\n",
       "       1.99722210e-01, 1.93668474e+00, 1.92536883e+00, 1.70891550e+00,\n",
       "       3.12199132e+00, 1.53635752e+00, 3.43024968e-01, 1.30126862e+00,\n",
       "       2.00549494e+00, 1.46829079e+00, 3.08981251e-01, 1.42518477e+00,\n",
       "       9.56105778e+00, 1.83081390e-03, 2.36612220e+00, 2.12610202e+00,\n",
       "       1.67090495e+00, 1.50960590e+00, 3.64683883e+00, 3.19696413e+00,\n",
       "       1.20119064e+00, 2.33034055e+00, 4.32461493e-01, 6.20235227e+00,\n",
       "       3.69646511e-01, 1.91258872e+00, 5.16426608e-02, 1.51964018e-01]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = trainer\n",
    "batch_data = batch\n",
    "\n",
    "from thexp import Meter\n",
    "from data.transforms import Int2Sent\n",
    "\n",
    "meter = Meter()\n",
    "int2sent_func = Int2Sent()\n",
    "\n",
    "\n",
    "if params.greedy_or_beam:\n",
    "    from thextra.inferences import sample_decode as decode_func\n",
    "else:\n",
    "    from thextra.inferences import beam_search_decode as decode_func\n",
    "\n",
    "enc_outs = self.model.encode(batch_data=batch_data)  # type:dict\n",
    "init_words = torch.zeros(batch_data['attn_masks'].size(0), dtype=torch.int64).to(self.device)\n",
    "\n",
    "states = []\n",
    "for _ in range(2):  # (hidden, cell)\n",
    "    states.append(torch.zeros((2, params.batch_size, self.params.hidden_size),\n",
    "                              dtype=torch.float32).to(self.device))\n",
    "\n",
    "_, max_attn_len = batch_data['attn_masks'].shape\n",
    "prev_attn_score = torch.zeros((params.batch_size, max_attn_len), device=self.device)\n",
    "prev_attn_score[:, 0] = 1\n",
    "\n",
    "enc_globals = enc_outs['init_states']\n",
    "enc_memories = enc_outs['attn_fts']\n",
    "enc_masks = batch_data['attn_masks']\n",
    "flow_edges = batch_data['flow_edges']\n",
    "\n",
    "result = decode_func(init_words, self.model.decoder.step_fn, params.max_words_in_sent,\n",
    "                     greedy=True, states=states, enc_globals=enc_globals,\n",
    "                     enc_memories=enc_memories, memory_masks=enc_masks,\n",
    "                     prev_attn_score=prev_attn_score, flow_edges=flow_edges)\n",
    "\n",
    "if params.greedy_or_beam:\n",
    "    pred_sent, word_logprobs = result\n",
    "    sent_pool = []\n",
    "    for sent, word_logprob in zip(pred_sent, word_logprobs):\n",
    "        sent_pool.append([(word_logprob.sum().item(), sent, word_logprob)])\n",
    "else:\n",
    "    sent_pool = result\n",
    "    pred_sent = [pool[0][1] for pool in sent]\n",
    "\n",
    "word_sents = []\n",
    "gt_sents = []\n",
    "for i, sent, gt_sent in zip(batch_data['index'], pred_sent, batch_data['caption_ids']):\n",
    "    sent = sent.detach().cpu().numpy().tolist()\n",
    "    gt_sent = gt_sent.detach().cpu().numpy().tolist()\n",
    "\n",
    "    sent = int2sent_func(sent)\n",
    "    gt_sent = int2sent_func(gt_sent)\n",
    "    gt_sents.append(gt_sent)\n",
    "    word_sents.append(sent)\n",
    "\n",
    "gt_sents = {i: [v] for i, v in enumerate(gt_sents)}\n",
    "word_sents = {i: [v] for i, v in enumerate(word_sents)}\n",
    "\n",
    "self.acc_bleu_(gt_sents, word_sents, meter=meter)\n",
    "self.acc_cider_(gt_sents, word_sents, meter=meter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "muslim-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "confused-electronics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage object at 0x7f87477b77d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(plt.imread(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "promising-black",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS> a counter of cakes surrounded by a group of people']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_sents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "conscious-pleasure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a table with a cake sitting on top of a wall in a room']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sents[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
